#!/usr/bin/env python3
"""
MLLMå¢å¼ºçš„Agenticè§†é¢‘ç›®æ ‡è·Ÿè¸ªç³»ç»Ÿ
é›†æˆå¤šæ¨¡æ€LLMè¿›è¡Œæ™ºèƒ½å‚æ•°å†³ç­–

ä½œè€…: Caroline Xia
ç‰ˆæœ¬: MLLM Enhanced
"""

import cv2
import argparse
from pathlib import Path
from agent.graph_build_mllm import build_mllm_app

def main():
    parser = argparse.ArgumentParser(description="MLLMå¢å¼ºçš„Agenticè§†é¢‘ç›®æ ‡è·Ÿè¸ªç³»ç»Ÿ")
    parser.add_argument("--video", type=str, required=True, help="è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--text", type=str, required=True, help="ç›®æ ‡æè¿°æ–‡æœ¬")
    parser.add_argument("--output_dir", type=str, default="./mllm_output", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # éªŒè¯è¾“å…¥
    video_path = Path(args.video)
    if not video_path.exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {args.video}")
        return
    
    print("ğŸš€ å¯åŠ¨MLLMå¢å¼ºçš„Agenticè§†é¢‘è·Ÿè¸ªç³»ç»Ÿ")
    print("="*60)
    print(f"ğŸ“¹ è¾“å…¥è§†é¢‘: {args.video}")
    print(f"ğŸ¯ è·Ÿè¸ªç›®æ ‡: {args.text}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print("="*60)
    
    # æ„å»ºMLLMåº”ç”¨
    app = build_mllm_app()
    
    # è·å–è§†é¢‘ä¿¡æ¯
    cap = cv2.VideoCapture(args.video)
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {total_frames} å¸§, {fps:.1f} FPS")
    
    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = {
        "video_path": args.video,
        "prompt": args.text,
        "user_input": args.text,
        "output_dir": args.output_dir,
        "fps": fps,
        "total_frames": total_frames,
        "frame_idx": 0,
        "gid_frames": {},
        "gid_scores": {},
        "missing_gap": 0,
        "stable": True,
        "need_human_review": False
    }
    
    print(f"\nğŸ¤– å¼€å§‹MLLMå‚æ•°åˆ†æ...")
    
    # é¦–å…ˆè¿›è¡ŒMLLMå‚æ•°åˆ†æï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
    mllm_state = initial_state.copy()
    mllm_state.update({
        "frame": None,  # åˆå§‹åŒ–æ—¶ä¸éœ€è¦frame
        "frame_idx": 0
    })
    
    # åªè°ƒç”¨MLLMç›¸å…³èŠ‚ç‚¹è¿›è¡Œå‚æ•°å†³ç­–
    from multimodal_advisor import MultiModalAdvisor
    
    try:
        API_KEY = "sk-ZKv84hrnoNgqB8bXy6qrIWga9ak4zHI1M7RcByXCLdRcoG1o"
        BASE_URL = "https://api.xinyun.ai/v1"
        advisor = MultiModalAdvisor(API_KEY, BASE_URL)
        
        result = advisor.analyze_video(args.video, args.text)
        llm_analysis = result.get('llm_analysis', {})
        
        if "error" not in llm_analysis:
            # æ£€æŸ¥ç›®æ ‡æ˜¯å¦å­˜åœ¨
            target_exists = llm_analysis.get("target_exists", True)
            target_found_frames = llm_analysis.get("target_found_frames", [])
            
            if not target_exists:
                print(f"   âŒ MLLMæ£€æµ‹ç»“æœ: è§†é¢‘ä¸­ä¸å­˜åœ¨ç›®æ ‡ç‰©ä½“")
                analysis = llm_analysis.get("analysis", {})
                reason = analysis.get("target_description_match", "æœªæä¾›åŸå› ")
                print(f"   ğŸ“ è¯¦ç»†åŸå› : {reason}")
                print(f"   ğŸš« å»ºè®®: è¯·æ£€æŸ¥ç›®æ ‡æè¿°æ˜¯å¦å‡†ç¡®ï¼Œæˆ–æ›´æ¢è§†é¢‘")
                
                # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
                print(f"\nâ“ æ˜¯å¦ä»è¦ç»§ç»­å¤„ç†? (è¾“å…¥ 'y' ç»§ç»­ï¼Œå…¶ä»–é”®é€€å‡º)")
                user_input = input().strip().lower()
                if user_input != 'y':
                    print(f"   â¹ï¸ ç”¨æˆ·é€‰æ‹©é€€å‡ºå¤„ç†")
                    cap.release()
                    return
                else:
                    print(f"   âš ï¸ ç”¨æˆ·é€‰æ‹©ç»§ç»­ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            
            # å¦‚æœç›®æ ‡å­˜åœ¨æˆ–ç”¨æˆ·é€‰æ‹©ç»§ç»­ï¼Œè®¾ç½®å‚æ•°
            if "recommended_params" in llm_analysis:
                params = llm_analysis["recommended_params"]
                analysis = llm_analysis.get("analysis", {})
                confidence = llm_analysis.get("confidence", 0.0)
                
                # æ›´æ–°å…¨å±€å‚æ•°
                initial_state.update({
                    "model_in_use": "yolo11n",  # æš‚æ—¶ä½¿ç”¨è½»é‡æ¨¡å‹
                    "clip_thresh": float(params.get("clip_thresh", 0.25)),
                    "max_bridge": int(params.get("max_bridge", 30)),
                    "gap": int(params.get("gap", 2)),
                    "track_buffer": int(params.get("track_buffer", 30)),
                    "match_thresh": float(params.get("match_thresh", 0.8)),
                    "track_thresh": float(params.get("track_thresh", 0.3)),
                    "mllm_confidence": float(confidence),
                    "mllm_reasoning": analysis.get("parameter_reasoning", "MLLMå‚æ•°æ¨è"),
                    "target_exists": target_exists,
                    "target_found_frames": target_found_frames
                })
                
                print(f"   âœ… MLLMå‚æ•°å†³ç­–å®Œæˆ:")
                print(f"      ç›®æ ‡å­˜åœ¨: {'æ˜¯' if target_exists else 'å¦'}")
                if target_found_frames:
                    print(f"      å‘ç°å¸§æ•°: {len(target_found_frames)}")
                print(f"      YOLOæ¨¡å‹: {initial_state['model_in_use']}")
                print(f"      CLIPé˜ˆå€¼: {params.get('clip_thresh', 0.25)}")
                print(f"      æ¡¥æ¥å‚æ•°: {params.get('max_bridge', 30)}")
                print(f"      è·Ÿè¸ªç¼“å†²: {params.get('track_buffer', 30)}")
                print(f"      ç½®ä¿¡åº¦: {confidence}")
            else:
                print(f"   âš ï¸ ç¼ºå°‘æ¨èå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
        else:
            print(f"   âš ï¸ MLLMåˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            
    except Exception as e:
        print(f"   âŒ MLLMåˆ†æå¼‚å¸¸: {e}")
        print(f"   ä½¿ç”¨é»˜è®¤å‚æ•°ç»§ç»­...")
    
    print(f"\nğŸ“¹ ä½¿ç”¨MLLMå‚æ•°å¯åŠ¨åŸå§‹ç³»ç»Ÿ...")
    
    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨MLLMå‚æ•°
    import tempfile
    import yaml
    
    temp_config = {
        "prompt": args.text,
        "default_model": initial_state.get("model_in_use", "yolo11n"),
        "clip": {
            "threshold": initial_state.get("clip_thresh", 0.25)
        },
        "bridge": {
            "default": initial_state.get("max_bridge", 30)
        },
        "detector": {
            "local": {
                "weights": "yolo11n.pt",
                "device": "cpu",
                "imgsz": 640
            }
        }
    }
    
    # å†™å…¥ä¸´æ—¶é…ç½®æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
        yaml.dump(temp_config, f, default_flow_style=False)
        temp_config_path = f.name
    
    # è°ƒç”¨åŸå§‹ç³»ç»Ÿ
    import subprocess
    import sys
    
    cmd = [
        sys.executable, "agent_main.py",
        "--video", args.video,
        "--config", temp_config_path,
        "--text", args.text,
        "--output_dir", args.output_dir
    ]
    
    print(f"   æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=False, text=True)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    import os
    os.unlink(temp_config_path)
    cap.release()
    
    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š MLLMå‚æ•°å†³ç­–ç»“æœ")
    print("="*60)
    
    if "mllm_confidence" in initial_state:
        print(f"ğŸ¤– MLLMç½®ä¿¡åº¦: {initial_state.get('mllm_confidence', 0.0)}")
        print(f"ğŸ“ å†³ç­–ç†ç”±: {initial_state.get('mllm_reasoning', 'N/A')}")
        print(f"\nğŸ“‹ æœ€ç»ˆä½¿ç”¨å‚æ•°:")
        print(f"   YOLOæ¨¡å‹: {initial_state.get('model_in_use', 'N/A')}")
        print(f"   CLIPé˜ˆå€¼: {initial_state.get('clip_thresh', 'N/A')}")
        print(f"   æ¡¥æ¥å‚æ•°: {initial_state.get('max_bridge', 'N/A')}")
        print(f"   é—´éš”å‚æ•°: {initial_state.get('gap', 'N/A')}")
        print(f"   è·Ÿè¸ªç¼“å†²: {initial_state.get('track_buffer', 'N/A')}")
        print(f"   åŒ¹é…é˜ˆå€¼: {initial_state.get('match_thresh', 'N/A')}")
        print(f"   è·Ÿè¸ªé˜ˆå€¼: {initial_state.get('track_thresh', 'N/A')}")
    
    # æ˜¾ç¤ºè·Ÿè¸ªç»Ÿè®¡
    gid_frames = initial_state.get("gid_frames", {})
    if gid_frames:
        print(f"\nğŸ¯ è·Ÿè¸ªç»Ÿè®¡:")
        for gid, frames in gid_frames.items():
            print(f"   Global ID {gid}: {len(frames)} å¸§")
    
    # æ˜¾ç¤ºå¯¼å‡ºç»“æœ
    exported_path = initial_state.get("exported_video_path")
    if exported_path:
        print(f"\nâœ… è§†é¢‘å¯¼å‡ºæˆåŠŸ: {exported_path}")
    else:
        export_error = initial_state.get("export_error")
        if export_error:
            print(f"\nâŒ å¯¼å‡ºå¤±è´¥: {export_error}")
    
    print("\nğŸ‰ MLLMå¢å¼ºå¤„ç†å®Œæˆ!")

if __name__ == "__main__":
    main()
