#!/usr/bin/env python3
"""
å¤šæ¨¡æ€LLMå‚æ•°é¡¾é—®ç³»ç»Ÿ
ç”¨äºåˆ†æè§†é¢‘å†…å®¹å¹¶æ¨èæœ€ä¼˜çš„è·Ÿè¸ªå‚æ•°

ä½œè€…: Caroline Xia
ç”¨é€”: å­¦æœ¯ç ”ç©¶ - Agentic AIè§†é¢‘ç›®æ ‡è·Ÿè¸ªç³»ç»Ÿ
"""

import cv2
import json
import base64
import requests
from typing import List, Dict, Any, Tuple
import numpy as np
from pathlib import Path
from datetime import datetime

class MultiModalAdvisor:
    """å¤šæ¨¡æ€LLMå‚æ•°é¡¾é—®"""
    
    def __init__(self, api_key: str, base_url: str = "https://api.xinyun.ai/v1"):
        self.api_key = api_key
        self.base_url = base_url
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
    
    def extract_key_frames(self, video_path: str, n_frames: int = 5) -> List[np.ndarray]:
        """æå–è§†é¢‘å…³é”®å¸§"""
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # å‡åŒ€åˆ†å¸ƒæå–å…³é”®å¸§
        frame_indices = np.linspace(0, total_frames - 1, n_frames, dtype=int)
        frames = []
        
        for idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                frames.append(frame)
        
        cap.release()
        return frames
    
    def get_video_stats(self, video_path: str) -> Dict[str, Any]:
        """è·å–è§†é¢‘åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
        cap = cv2.VideoCapture(video_path)
        
        stats = {
            "duration": cap.get(cv2.CAP_PROP_FRAME_COUNT) / cap.get(cv2.CAP_PROP_FPS),
            "fps": cap.get(cv2.CAP_PROP_FPS),
            "width": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
            "height": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
            "total_frames": int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        }
        
        cap.release()
        return stats
    
    def frame_to_base64(self, frame: np.ndarray) -> str:
        """å°†å¸§è½¬æ¢ä¸ºbase64ç¼–ç """
        # è°ƒæ•´å›¾ç‰‡å¤§å°ä»¥å‡å°‘APIè°ƒç”¨æˆæœ¬
        height, width = frame.shape[:2]
        if width > 800:
            scale = 800 / width
            new_width = 800
            new_height = int(height * scale)
            frame = cv2.resize(frame, (new_width, new_height))
        
        _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
        return base64.b64encode(buffer).decode('utf-8')
    
    def analyze_video_complexity(self, frames: List[np.ndarray]) -> Dict[str, float]:
        """åˆ†æè§†é¢‘å¤æ‚åº¦ç‰¹å¾"""
        complexity_metrics = {
            "motion_intensity": 0.0,
            "background_complexity": 0.0,
            "brightness_variation": 0.0,
            "edge_density": 0.0
        }
        
        if len(frames) < 2:
            return complexity_metrics
        
        # è®¡ç®—å¸§é—´å·®å¼‚ï¼ˆè¿åŠ¨å¼ºåº¦ï¼‰
        motion_scores = []
        for i in range(1, len(frames)):
            gray1 = cv2.cvtColor(frames[i-1], cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
            diff = cv2.absdiff(gray1, gray2)
            motion_scores.append(np.mean(diff))
        
        complexity_metrics["motion_intensity"] = np.mean(motion_scores) / 255.0
        
        # è®¡ç®—èƒŒæ™¯å¤æ‚åº¦ï¼ˆè¾¹ç¼˜å¯†åº¦ï¼‰
        edge_scores = []
        brightness_scores = []
        
        for frame in frames:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # è¾¹ç¼˜å¯†åº¦
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])
            edge_scores.append(edge_density)
            
            # äº®åº¦å˜åŒ–
            brightness_scores.append(np.std(gray) / 255.0)
        
        complexity_metrics["edge_density"] = np.mean(edge_scores)
        complexity_metrics["background_complexity"] = np.mean(edge_scores)
        complexity_metrics["brightness_variation"] = np.mean(brightness_scores)
        
        return complexity_metrics
    
    def call_multimodal_llm(self, frames: List[np.ndarray], prompt: str, video_stats: Dict, complexity: Dict) -> Dict[str, Any]:
        """è°ƒç”¨å¤šæ¨¡æ€LLMåˆ†æè§†é¢‘å¹¶æ¨èå‚æ•°"""
        
        # å‡†å¤‡å›¾ç‰‡æ•°æ®
        image_contents = []
        for i, frame in enumerate(frames[:3]):  # é™åˆ¶ä¸º3å¸§ä»¥æ§åˆ¶æˆæœ¬
            base64_image = self.frame_to_base64(frame)
            image_contents.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_image}"
                }
            })
        
        # æ„å»ºLLMæç¤º
        llm_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘åˆ†æä¸“å®¶ï¼Œä¸“é—¨ä¸ºè§†é¢‘ç›®æ ‡è·Ÿè¸ªç³»ç»Ÿæ¨èæœ€ä¼˜å‚æ•°ã€‚

## ä»»åŠ¡æè¿°
è·Ÿè¸ªç›®æ ‡: {prompt}

## è§†é¢‘ä¿¡æ¯
- æ—¶é•¿: {video_stats['duration']:.1f}ç§’
- å¸§ç‡: {video_stats['fps']:.1f} FPS  
- åˆ†è¾¨ç‡: {video_stats['width']}x{video_stats['height']}
- æ€»å¸§æ•°: {video_stats['total_frames']}

## è®¡ç®—æœºè§†è§‰åˆ†æ
- è¿åŠ¨å¼ºåº¦: {complexity['motion_intensity']:.3f}
- èƒŒæ™¯å¤æ‚åº¦: {complexity['background_complexity']:.3f}
- äº®åº¦å˜åŒ–: {complexity['brightness_variation']:.3f}
- è¾¹ç¼˜å¯†åº¦: {complexity['edge_density']:.3f}

## å‚æ•°è¯´æ˜
è¯·åŸºäºè§†é¢‘å†…å®¹åˆ†æï¼Œä¸ºä»¥ä¸‹å‚æ•°æ¨èæœ€ä¼˜å€¼ï¼š

1. **clip_thresh** (0.15-0.35): CLIPè¯­ä¹‰åŒ¹é…é˜ˆå€¼
   - è¶Šä½è¶Šå®½æ¾ï¼Œå®¹æ˜“åŒ¹é…ä½†å¯èƒ½è¯¯åŒ¹é…
   - è¶Šé«˜è¶Šä¸¥æ ¼ï¼Œå‡†ç¡®ä½†å¯èƒ½æ¼æ£€

2. **max_bridge** (20-80): ç‰‡æ®µæ¡¥æ¥æœ€å¤§é—´éš”ï¼ˆå¸§æ•°ï¼‰
   - ç”¨äºè¿æ¥è¢«çŸ­æš‚é®æŒ¡çš„ç›®æ ‡ç‰‡æ®µ
   - è¿åŠ¨è¿ç»­æ€§å¥½å¯ä»¥è®¾å¤§ä¸€äº›

3. **gap** (1-5): å¸§å†…è¿ç»­æ€§é—´éš”
   - åŒä¸€ç‰‡æ®µå†…å…è®¸çš„æœ€å¤§æ£€æµ‹é—´éš”
   - æ£€æµ‹ç¨³å®šå¯ä»¥è®¾å¤§ä¸€äº›

4. **track_buffer** (15-50): è·Ÿè¸ªç¼“å†²åŒºå¤§å°
   - ç›®æ ‡æ¶ˆå¤±åä¿æŒå¤šä¹…çš„è·Ÿè¸ªè®°å¿†
   - é®æŒ¡é¢‘ç¹éœ€è¦è®¾å¤§ä¸€äº›

5. **match_thresh** (0.6-0.9): ByteTrackåŒ¹é…é˜ˆå€¼
   - æ–°æ£€æµ‹ä¸å·²æœ‰è½¨è¿¹çš„åŒ¹é…ä¸¥æ ¼ç¨‹åº¦

6. **track_thresh** (0.2-0.5): è·Ÿè¸ªç½®ä¿¡åº¦é˜ˆå€¼
   - ä½äºæ­¤å€¼çš„æ£€æµ‹ä¸å‚ä¸è·Ÿè¸ª

## åˆ†æè¦æ±‚
è¯·ä»”ç»†è§‚å¯Ÿæä¾›çš„å…³é”®å¸§ï¼ŒæŒ‰ä»¥ä¸‹æ­¥éª¤è¿›è¡Œåˆ†æï¼š

**ç¬¬ä¸€æ­¥ï¼šç›®æ ‡å­˜åœ¨æ€§æ£€æµ‹**
- ä»”ç»†è§‚å¯Ÿæ¯ä¸€å¸§ï¼Œåˆ¤æ–­æè¿°çš„ç›®æ ‡æ˜¯å¦çœŸå®å­˜åœ¨äºè§†é¢‘ä¸­
- å¦‚æœå­˜åœ¨ï¼Œæ ‡è®°åœ¨å“ªäº›å¸§ä¸­å‘ç°äº†ç›®æ ‡
- è¯„ä¼°ç›®æ ‡æè¿°ä¸å®é™…å†…å®¹çš„åŒ¹é…ç¨‹åº¦

**ç¬¬äºŒæ­¥ï¼šç›®æ ‡ç‰¹å¾åˆ†æ**ï¼ˆä»…å½“ç›®æ ‡å­˜åœ¨æ—¶ï¼‰
1. **ç›®æ ‡ç‰¹å¾**: ç›®æ ‡å¯¹è±¡çš„æ¸…æ™°åº¦ã€é¢œè‰²ã€å½¢çŠ¶ç‰¹å¾æ˜¯å¦æ˜æ˜¾ï¼Ÿ
2. **ç›¸ä¼¼å¹²æ‰°**: åœºæ™¯ä¸­æ˜¯å¦æœ‰ä¸ç›®æ ‡ç›¸ä¼¼çš„å…¶ä»–å¯¹è±¡ï¼Ÿ
3. **è¿åŠ¨æ¨¡å¼**: ç›®æ ‡è¿åŠ¨æ˜¯å¦å¹³æ»‘è¿ç»­ï¼Ÿæ˜¯å¦æœ‰æ€¥å‰§å˜åŒ–ï¼Ÿ
4. **é®æŒ¡æƒ…å†µ**: æ˜¯å¦å­˜åœ¨é®æŒ¡ï¼Ÿé®æŒ¡æŒç»­æ—¶é—´å¦‚ä½•ï¼Ÿ
5. **èƒŒæ™¯å¹²æ‰°**: èƒŒæ™¯æ˜¯å¦å¤æ‚ï¼Ÿæ˜¯å¦å½±å“ç›®æ ‡æ£€æµ‹ï¼Ÿ
6. **å…‰ç…§æ¡ä»¶**: å…‰ç…§æ˜¯å¦ç¨³å®šï¼Ÿæ˜¯å¦æœ‰æ˜æš—å˜åŒ–ï¼Ÿ

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š

```json
{{
    "target_exists": true/false,
    "target_found_frames": [1,2,3],
    "recommended_params": {{
        "clip_thresh": 0.xx,
        "max_bridge": xx,
        "gap": x,
        "track_buffer": xx,
        "match_thresh": 0.xx,
        "track_thresh": 0.xx
    }},
    "analysis": {{
        "target_description_match": "ç›®æ ‡æè¿°ä¸è§†é¢‘å†…å®¹çš„åŒ¹é…æƒ…å†µ",
        "target_clarity": "ç›®æ ‡æ¸…æ™°åº¦åˆ†æ",
        "scene_complexity": "åœºæ™¯å¤æ‚åº¦åˆ†æ", 
        "motion_pattern": "è¿åŠ¨æ¨¡å¼åˆ†æ",
        "occlusion_assessment": "é®æŒ¡æƒ…å†µè¯„ä¼°",
        "tracking_difficulty": "è·Ÿè¸ªéš¾åº¦è¯„ä¼° (easy/medium/hard)",
        "parameter_reasoning": "å‚æ•°é€‰æ‹©è¯¦ç»†ç†ç”±"
    }},
    "confidence": 0.xx
}}
```

**é‡è¦æé†’**ï¼š
- å¦‚æœtarget_existsä¸ºfalseï¼Œè¯·åœ¨analysisä¸­è¯¦ç»†è¯´æ˜æœªæ‰¾åˆ°ç›®æ ‡çš„åŸå› 
- åªæœ‰å½“ç›®æ ‡ç¡®å®å­˜åœ¨æ—¶ï¼Œæ‰éœ€è¦æ¨èè·Ÿè¸ªå‚æ•°
- è¯·ä»”ç»†è§‚å¯Ÿæ¯ä¸€å¸§çš„ç»†èŠ‚ï¼Œä¸è¦åŒ†å¿™ä¸‹ç»“è®º
"""

        # æ„å»ºAPIè¯·æ±‚
        messages = [
            {
                "role": "user", 
                "content": [
                    {"type": "text", "text": llm_prompt}
                ] + image_contents
            }
        ]
        
        payload = {
            "model": "gemini-1.5-flash",  # ä½¿ç”¨æ›´ç¨³å®šçš„flashç‰ˆæœ¬
            "messages": messages,
            "max_tokens": 1000,
            "temperature": 0.1
        }
        
        try:
            print(f"   å‘é€APIè¯·æ±‚åˆ°: {self.base_url}/chat/completions")
            print(f"   ä½¿ç”¨æ¨¡å‹: {payload['model']}")
            print(f"   å›¾ç‰‡æ•°é‡: {len(image_contents)}")
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=self.headers,
                json=payload,
                timeout=60
            )
            
            print(f"   APIå“åº”çŠ¶æ€: {response.status_code}")
            
            if response.status_code != 200:
                error_msg = f"HTTP {response.status_code}: {response.text[:500]}"
                print(f"   âŒ APIé”™è¯¯: {error_msg}")
                return {"error": error_msg}
            
            result = response.json()
            llm_response = result['choices'][0]['message']['content']
            
            print(f"   âœ… APIè°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(llm_response)} å­—ç¬¦")
            
            # å°è¯•è§£æJSONå“åº”
            try:
                # æå–JSONéƒ¨åˆ†
                json_start = llm_response.find('{')
                json_end = llm_response.rfind('}') + 1
                if json_start != -1 and json_end != -1:
                    json_str = llm_response[json_start:json_end]
                    parsed_json = json.loads(json_str)
                    print(f"   âœ… JSONè§£ææˆåŠŸ")
                    
                    # éªŒè¯å¹¶æ·»åŠ é»˜è®¤å€¼
                    if "target_exists" not in parsed_json:
                        print(f"   âš ï¸ ç¼ºå°‘target_existså­—æ®µï¼Œé»˜è®¤è®¾ä¸ºtrue")
                        parsed_json["target_exists"] = True
                    
                    if "target_found_frames" not in parsed_json:
                        parsed_json["target_found_frames"] = []
                    
                    # æ˜¾ç¤ºç›®æ ‡æ£€æµ‹ç»“æœ
                    target_exists = parsed_json.get("target_exists", True)
                    if target_exists:
                        found_frames = parsed_json.get("target_found_frames", [])
                        print(f"   ğŸ¯ ç›®æ ‡æ£€æµ‹: å­˜åœ¨ (å‘ç°äºå¸§: {found_frames})")
                    else:
                        print(f"   âš ï¸ ç›®æ ‡æ£€æµ‹: ä¸å­˜åœ¨")
                        reason = parsed_json.get("analysis", {}).get("target_description_match", "æœªæä¾›åŸå› ")
                        print(f"   ğŸ“ åŸå› : {reason}")
                    
                    return parsed_json
            except Exception as parse_error:
                print(f"   âš ï¸ JSONè§£æå¤±è´¥: {str(parse_error)}")
                print(f"   åŸå§‹å“åº”: {llm_response[:300]}...")
            
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å“åº”
            return {"raw_response": llm_response, "error": "JSON parsing failed"}
            
        except requests.exceptions.RequestException as e:
            error_msg = f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"
            print(f"   âŒ {error_msg}")
            return {"error": error_msg}
        except Exception as e:
            error_msg = f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}"
            print(f"   âŒ {error_msg}")
            return {"error": error_msg}
    
    def analyze_video(self, video_path: str, prompt: str) -> Dict[str, Any]:
        """å®Œæ•´çš„è§†é¢‘åˆ†ææµç¨‹"""
        print(f"ğŸ¬ å¼€å§‹åˆ†æè§†é¢‘: {video_path}")
        print(f"ğŸ¯ è·Ÿè¸ªç›®æ ‡: {prompt}")
        
        # 1. æå–å…³é”®å¸§
        print("ğŸ“¸ æå–å…³é”®å¸§...")
        frames = self.extract_key_frames(video_path)
        print(f"   æå–äº† {len(frames)} ä¸ªå…³é”®å¸§")
        
        # 2. è·å–è§†é¢‘ç»Ÿè®¡
        print("ğŸ“Š åˆ†æè§†é¢‘ç»Ÿè®¡ä¿¡æ¯...")
        video_stats = self.get_video_stats(video_path)
        print(f"   æ—¶é•¿: {video_stats['duration']:.1f}s, å¸§ç‡: {video_stats['fps']:.1f} FPS")
        
        # 3. è®¡ç®—å¤æ‚åº¦æŒ‡æ ‡
        print("ğŸ§® è®¡ç®—å¤æ‚åº¦æŒ‡æ ‡...")
        complexity = self.analyze_video_complexity(frames)
        
        # 4. è°ƒç”¨LLMåˆ†æ
        print("ğŸ¤– è°ƒç”¨å¤šæ¨¡æ€LLMåˆ†æ...")
        llm_result = self.call_multimodal_llm(frames, prompt, video_stats, complexity)
        
        # 5. æ•´åˆç»“æœ
        result = {
            "video_path": video_path,
            "prompt": prompt,
            "video_stats": video_stats,
            "complexity_metrics": complexity,
            "llm_analysis": llm_result,
            "timestamp": str(pd.Timestamp.now()) if 'pd' in globals() else "unknown"
        }
        
        return result

def test_multiple_videos():
    """æµ‹è¯•å¤šä¸ªdemoè§†é¢‘"""
    # APIé…ç½® - ç›´æ¥å†™å…¥ä»£ç ç”¨äºæµ‹è¯•
    API_KEY = "sk-ZKv84hrnoNgqB8bXy6qrIWga9ak4zHI1M7RcByXCLdRcoG1o"
    BASE_URL = "https://api.xinyun.ai/v1"
    
    print(f"ğŸ”‘ ä½¿ç”¨API Key: {API_KEY[:20]}...")
    print(f"ğŸŒ APIåœ°å€: {BASE_URL}")
    
    # åˆå§‹åŒ–é¡¾é—®
    advisor = MultiModalAdvisor(API_KEY, BASE_URL)
    
    # å®šä¹‰æµ‹è¯•åœºæ™¯
    test_scenarios = [
        {
            "video_path": "demo.mp4",
            "prompt": "track the brown teddy dog",
            "scenario_name": "Demo1: æ£•è‰²æ³°è¿ªçŠ¬"
        },
        {
            "video_path": "demo1.mp4", 
            "prompt": "track the person in dark clothing",
            "scenario_name": "Demo1: æ·±è‰²è¡£æœçš„äºº"
        },
        {
            "video_path": "demo2.mp4",
            "prompt": "track the woman in white clothes dancing",  # æ ¹æ®æ‚¨çš„æè¿°æ›´æ–°
            "scenario_name": "Demo2: ç™½è¡£è·³èˆå¥³æ€§"
        },
        {
            "video_path": "demo3.mp4",
            "prompt": "track the man in white shirt",  # æ ¹æ®æ‚¨çš„æè¿°æ›´æ–°
            "scenario_name": "Demo3: ç™½è¡£ç”·æ€§"
        }
    ]
    
    results = []
    
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯• {len(test_scenarios)} ä¸ªè§†é¢‘åœºæ™¯")
    print("="*80)
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"\nğŸ”„ è¿›åº¦: {i}/{len(test_scenarios)}")
        print(f"ğŸ¬ åœºæ™¯: {scenario['scenario_name']}")
        print(f"ğŸ“¹ è§†é¢‘: {scenario['video_path']}")
        print(f"ğŸ¯ ç›®æ ‡: {scenario['prompt']}")
        
        if not Path(scenario['video_path']).exists():
            print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {scenario['video_path']}")
            continue
        
        # æ‰§è¡Œåˆ†æ
        result = advisor.analyze_video(scenario['video_path'], scenario['prompt'])
        result['scenario_name'] = scenario['scenario_name']
        results.append(result)
        
        # æ˜¾ç¤ºç®€è¦ç»“æœ
        display_brief_result(result, scenario['scenario_name'])
        
        print("-" * 60)
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    if results:
        generate_comparison_report(results)
    
    return results

def display_brief_result(result: dict, scenario_name: str):
    """æ˜¾ç¤ºç®€è¦ç»“æœ"""
    llm_analysis = result.get('llm_analysis', {})
    
    if "error" in llm_analysis:
        print(f"âŒ åˆ†æå¤±è´¥: {llm_analysis['error']}")
        return
    
    # æ˜¾ç¤ºæ¨èå‚æ•°
    if "recommended_params" in llm_analysis:
        params = llm_analysis["recommended_params"]
        print(f"ğŸ“‹ æ¨èå‚æ•°:")
        print(f"   clip_thresh: {params.get('clip_thresh', 'N/A')}")
        print(f"   max_bridge: {params.get('max_bridge', 'N/A')}")
        print(f"   track_buffer: {params.get('track_buffer', 'N/A')}")
    
    # æ˜¾ç¤ºç½®ä¿¡åº¦
    confidence = llm_analysis.get('confidence', 0)
    print(f"ğŸ“Š ç½®ä¿¡åº¦: {confidence}")

def generate_comparison_report(results: list):
    """ç”Ÿæˆå¤šè§†é¢‘å¯¹æ¯”æŠ¥å‘Š"""
    print(f"\n{'='*80}")
    print("ğŸ“Š å¤šè§†é¢‘å‚æ•°å¯¹æ¯”æŠ¥å‘Š")
    print('='*80)
    
    successful_results = [r for r in results if "error" not in r.get('llm_analysis', {})]
    
    if not successful_results:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„åˆ†æç»“æœ")
        return
    
    print(f"âœ… æˆåŠŸåˆ†æ: {len(successful_results)}/{len(results)} ä¸ªè§†é¢‘")
    
    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    print(f"\nğŸ“‹ å‚æ•°å¯¹æ¯”è¡¨:")
    print(f"{'åœºæ™¯':<20} {'clip_thresh':<12} {'max_bridge':<12} {'track_buffer':<13} {'ç½®ä¿¡åº¦':<8}")
    print("-" * 70)
    
    for result in successful_results:
        scenario = result.get('scenario_name', 'Unknown')[:18]
        llm_analysis = result.get('llm_analysis', {})
        
        if "recommended_params" in llm_analysis:
            params = llm_analysis["recommended_params"]
            confidence = llm_analysis.get('confidence', 0)
            
            clip_thresh = params.get('clip_thresh', 'N/A')
            max_bridge = params.get('max_bridge', 'N/A')
            track_buffer = params.get('track_buffer', 'N/A')
            
            print(f"{scenario:<20} {clip_thresh:<12} {max_bridge:<12} {track_buffer:<13} {confidence:<8}")
    
    # åˆ†æè¶‹åŠ¿
    print(f"\nğŸ” å‚æ•°è¶‹åŠ¿åˆ†æ:")
    
    param_values = {'clip_thresh': [], 'max_bridge': [], 'track_buffer': []}
    
    for result in successful_results:
        llm_analysis = result.get('llm_analysis', {})
        if "recommended_params" in llm_analysis:
            params = llm_analysis["recommended_params"]
            for param in param_values:
                value = params.get(param)
                if value is not None and isinstance(value, (int, float)):
                    param_values[param].append(value)
    
    for param, values in param_values.items():
        if values:
            avg_val = sum(values) / len(values)
            min_val = min(values)
            max_val = max(values)
            print(f"   {param}: å¹³å‡={avg_val:.3f}, èŒƒå›´=[{min_val:.3f}, {max_val:.3f}]")

def main():
    """ä¸»å‡½æ•° - é€‰æ‹©å•è§†é¢‘æµ‹è¯•è¿˜æ˜¯å¤šè§†é¢‘æµ‹è¯•"""
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--multi":
        # å¤šè§†é¢‘æµ‹è¯•
        results = test_multiple_videos()
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S") if 'datetime' in globals() else "test"
        output_file = f"multi_video_analysis_{timestamp}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    else:
        # å•è§†é¢‘æµ‹è¯•ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰
        single_video_test()

def single_video_test():
    """å•è§†é¢‘æµ‹è¯•ï¼ˆåŸæœ‰åŠŸèƒ½ï¼‰"""
    # APIé…ç½® - ç›´æ¥å†™å…¥ä»£ç ç”¨äºæµ‹è¯•
    API_KEY = "sk-ZKv84hrnoNgqB8bXy6qrIWga9ak4zHI1M7RcByXCLdRcoG1o"
    BASE_URL = "https://api.xinyun.ai/v1"
    
    # åˆå§‹åŒ–é¡¾é—®
    advisor = MultiModalAdvisor(API_KEY, BASE_URL)
    
    # åˆ†ædemoè§†é¢‘
    video_path = "demo.mp4"  # æ‚¨çš„è§†é¢‘è·¯å¾„
    prompt = "track the brown teddy dog"  # æ‚¨çš„è·Ÿè¸ªç›®æ ‡
    
    if not Path(video_path).exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        return
    
    # æ‰§è¡Œåˆ†æ
    result = advisor.analyze_video(video_path, prompt)
    
    # è¾“å‡ºç»“æœ
    print("\n" + "="*60)
    print("ğŸ¯ å¤šæ¨¡æ€LLMå‚æ•°å»ºè®®")
    print("="*60)
    
    if "error" in result["llm_analysis"]:
        print(f"âŒ åˆ†æå¤±è´¥: {result['llm_analysis']['error']}")
        return
    
    # æ˜¾ç¤ºæ¨èå‚æ•°
    if "recommended_params" in result["llm_analysis"]:
        params = result["llm_analysis"]["recommended_params"]
        print("\nğŸ“‹ æ¨èå‚æ•°:")
        for key, value in params.items():
            print(f"   {key}: {value}")
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    if "analysis" in result["llm_analysis"]:
        analysis = result["llm_analysis"]["analysis"]
        print("\nğŸ” åˆ†æç»“æœ:")
        for key, value in analysis.items():
            print(f"   {key}: {value}")
    
    # æ˜¾ç¤ºç½®ä¿¡åº¦
    if "confidence" in result["llm_analysis"]:
        confidence = result["llm_analysis"]["confidence"]
        print(f"\nğŸ“Š åˆ†æç½®ä¿¡åº¦: {confidence}")
    
    # å¯¹æ¯”å½“å‰ä½¿ç”¨çš„å‚æ•°
    print("\n" + "="*60)
    print("ğŸ“Š å‚æ•°å¯¹æ¯”")
    print("="*60)
    
    current_params = {
        "clip_thresh": 0.25,
        "max_bridge": 50,  # æ‚¨å½“å‰ä¼˜åŒ–åçš„å€¼
        "gap": 2,
        "track_buffer": 30,
        "match_thresh": 0.8,
        "track_thresh": 0.3
    }
    
    if "recommended_params" in result["llm_analysis"]:
        recommended = result["llm_analysis"]["recommended_params"]
        print(f"{'å‚æ•°':<15} {'å½“å‰å€¼':<10} {'å»ºè®®å€¼':<10} {'å·®å¼‚':<10}")
        print("-" * 50)
        
        for param in current_params:
            current = current_params[param]
            suggested = recommended.get(param, "N/A")
            
            if suggested != "N/A":
                diff = f"{suggested - current:+.3f}" if isinstance(suggested, (int, float)) else "N/A"
            else:
                diff = "N/A"
                
            print(f"{param:<15} {current:<10} {suggested:<10} {diff:<10}")
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    output_file = "multimodal_analysis_result.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ å®Œæ•´åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    main()
