from langgraph.graph import StateGraph, START, END
from agent.state_types import AgentState
from adapter.nlp_parser import nlp_json_deepseek
from adapter.detection_api import DetectorAPI, yolo_to_boxes_api 
from adapter.tracking import run_bytetrack
from adapter.clip_matcher import clip_stats
from adapter.editing import run_video, merge_frame_ranges, export_best_match_video
from adapter.global_id import update_stats, is_ready, get_or_assign_gid_with_reuse, get_assigned_gid
from multimodal_advisor import MultiModalAdvisor
import torch
from pathlib import Path

def build_mllm_app():
    """æ„å»ºé›†æˆMLLMå‚æ•°å†³ç­–çš„åº”ç”¨"""
    g = StateGraph(dict)
    
    # ---------- èŠ‚ç‚¹å‡½æ•° ----------
    def n_parse(s):
        """NLPè§£æèŠ‚ç‚¹"""
        if s.get("parsed"):
            return s
        p = nlp_json_deepseek(s.get("user_input") or s.get("prompt", ""))
        s["parsed"] = p
        s["prompt"] = p.get("clip_prompt", s.get("prompt", ""))
        s["det_class_hint"] = p.get("yolo_class")
        s["need_human_review"] = bool(p.get("parse_error"))
        print("Parsed:", p)
        print(f"clip_prompt={s['prompt']}  yolo_class={s.get('det_class_hint')}")
        return s

    def n_mllm_advisor(s):
        """MLLMå‚æ•°é¡¾é—®èŠ‚ç‚¹ - ç»Ÿä¸€å†³å®šæ‰€æœ‰å‚æ•°"""
        # åªåœ¨ç¬¬ä¸€å¸§æ‰§è¡ŒMLLMåˆ†æï¼Œé¿å…é‡å¤è°ƒç”¨
        if s.get("mllm_analyzed", False):
            return s
        
        video_path = s.get("video_path", "")
        prompt = s.get("prompt", "")
        
        print(f"ğŸ¤– [MLLM] å¼€å§‹è§†é¢‘åˆ†æå’Œå‚æ•°å†³ç­–...")
        print(f"   è§†é¢‘: {video_path}")
        print(f"   ç›®æ ‡: {prompt}")
        
        if not Path(video_path).exists():
            print(f"   âš ï¸ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            # ä½¿ç”¨é»˜è®¤å‚æ•°
            s.update({
                "model_in_use": "yolo11n",
                "clip_thresh": 0.25,
                "max_bridge": 30,
                "gap": 2,
                "track_buffer": 30,
                "match_thresh": 0.8,
                "track_thresh": 0.3,
                "mllm_confidence": 0.0,
                "mllm_reasoning": "è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°",
                "mllm_analyzed": True
            })
            return s
        
        try:
            # åˆå§‹åŒ–MLLMé¡¾é—®
            API_KEY = "sk-ZKv84hrnoNgqB8bXy6qrIWga9ak4zHI1M7RcByXCLdRcoG1o"
            BASE_URL = "https://api.xinyun.ai/v1"
            advisor = MultiModalAdvisor(API_KEY, BASE_URL)
            
            # æ‰§è¡Œè§†é¢‘åˆ†æ
            result = advisor.analyze_video(video_path, prompt)
            llm_analysis = result.get('llm_analysis', {})
            
            if "error" in llm_analysis:
                print(f"   âŒ MLLMåˆ†æå¤±è´¥: {llm_analysis['error']}")
                # ä½¿ç”¨é»˜è®¤å‚æ•°
                s.update({
                    "model_in_use": "yolo11n",
                    "clip_thresh": 0.25,
                    "max_bridge": 30,
                    "gap": 2,
                    "track_buffer": 30,
                    "match_thresh": 0.8,
                    "track_thresh": 0.3,
                    "mllm_confidence": 0.0,
                    "mllm_reasoning": f"MLLMåˆ†æå¤±è´¥: {llm_analysis.get('error', 'Unknown error')}"
                })
                return s
            
            # æå–MLLMæ¨èçš„å‚æ•°
            if "recommended_params" in llm_analysis:
                params = llm_analysis["recommended_params"]
                analysis = llm_analysis.get("analysis", {})
                confidence = llm_analysis.get("confidence", 0.0)
                
                # YOLOæ¨¡å‹é€‰æ‹©é€»è¾‘ï¼ˆåŸºäºåœºæ™¯å¤æ‚åº¦ï¼‰
                video_stats = result.get('video_stats', {})
                complexity = result.get('complexity_metrics', {})
                
                # æ ¹æ®è§†é¢‘ç‰¹å¾é€‰æ‹©YOLOæ¨¡å‹
                motion_intensity = complexity.get('motion_intensity', 0.0)
                background_complexity = complexity.get('background_complexity', 0.0)
                duration = video_stats.get('duration', 0.0)
                
                # YOLOæ¨¡å‹é€‰æ‹©ç­–ç•¥
                if motion_intensity > 0.3 or background_complexity > 0.4 or duration > 60:
                    yolo_model = "yolo11m"  # å¤æ‚åœºæ™¯ä½¿ç”¨ä¸­ç­‰æ¨¡å‹
                    if motion_intensity > 0.5 or background_complexity > 0.6:
                        yolo_model = "yolo11l"  # éå¸¸å¤æ‚åœºæ™¯ä½¿ç”¨å¤§æ¨¡å‹
                else:
                    yolo_model = "yolo11n"  # ç®€å•åœºæ™¯ä½¿ç”¨è½»é‡æ¨¡å‹
                
                # æ›´æ–°çŠ¶æ€
                s.update({
                    "model_in_use": yolo_model,
                    "clip_thresh": float(params.get("clip_thresh", 0.25)),
                    "max_bridge": int(params.get("max_bridge", 30)),
                    "gap": int(params.get("gap", 2)),
                    "track_buffer": int(params.get("track_buffer", 30)),
                    "match_thresh": float(params.get("match_thresh", 0.8)),
                    "track_thresh": float(params.get("track_thresh", 0.3)),
                    "mllm_confidence": float(confidence),
                    "mllm_reasoning": analysis.get("parameter_reasoning", "MLLMå‚æ•°æ¨è"),
                    "mllm_analysis": analysis,
                    "mllm_analyzed": True
                })
                
                print(f"   âœ… MLLMå‚æ•°å†³ç­–å®Œæˆ:")
                print(f"      YOLOæ¨¡å‹: {yolo_model}")
                print(f"      CLIPé˜ˆå€¼: {params.get('clip_thresh', 0.25)}")
                print(f"      æ¡¥æ¥å‚æ•°: {params.get('max_bridge', 30)}")
                print(f"      è·Ÿè¸ªç¼“å†²: {params.get('track_buffer', 30)}")
                print(f"      ç½®ä¿¡åº¦: {confidence}")
                
            else:
                print(f"   âš ï¸ MLLMæœªè¿”å›å‚æ•°å»ºè®®ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                s.update({
                    "model_in_use": "yolo11n",
                    "clip_thresh": 0.25,
                    "max_bridge": 30,
                    "gap": 2,
                    "track_buffer": 30,
                    "match_thresh": 0.8,
                    "track_thresh": 0.3,
                    "mllm_confidence": 0.0,
                    "mllm_reasoning": "MLLMæœªè¿”å›æœ‰æ•ˆå‚æ•°å»ºè®®",
                    "mllm_analyzed": True
                })
                
        except Exception as e:
            print(f"   âŒ MLLMèŠ‚ç‚¹å¼‚å¸¸: {str(e)}")
            # ä½¿ç”¨é»˜è®¤å‚æ•°
            s.update({
                "model_in_use": "yolo11n",
                "clip_thresh": 0.25,
                "max_bridge": 30,
                "gap": 2,
                "track_buffer": 30,
                "match_thresh": 0.8,
                "track_thresh": 0.3,
                "mllm_confidence": 0.0,
                "mllm_reasoning": f"MLLMèŠ‚ç‚¹å¼‚å¸¸: {str(e)}",
                "mllm_analyzed": True
            })
        
        return s

    def n_detect(s):
        """YOLOæ£€æµ‹èŠ‚ç‚¹"""
        frame_idx = s.get("frame_idx", 0)
        frame = s.get("frame")
        model_name = s.get("model_in_use", "yolo11n")
        det_class_hint = s.get("det_class_hint")
        
        if frame is None:
            s["detections"] = []
            s["det_found"] = False
            return s
        
        # ä½¿ç”¨MLLMå†³å®šçš„æ¨¡å‹ - åˆ›å»ºæœ¬åœ°æ£€æµ‹å™¨
        from adapter.detection_api import LocalYoloDetector
        
        # ç®€åŒ–çš„æ¨¡å‹æ˜ å°„
        alias2weights = {
            "yolo11n": "yolo11n.pt",
            "yolo11m": "yolo11m.pt", 
            "yolo11l": "yolo11l.pt",
            "yolo11x": "yolo11x.pt"
        }
        
        detector = LocalYoloDetector(alias2weights)
        raw_dets = detector.detect(frame, alias=model_name)
        
        # ç›´æ¥å¤„ç†raw_detsï¼Œå› ä¸ºLocalYoloDetectorå·²ç»è¿”å›äº†æ­£ç¡®æ ¼å¼
        detections = []
        for d in raw_dets or []:
            detections.append({
                "bbox": d.get("bbox", [0, 0, 0, 0]),
                "confidence": float(d.get("score", 0.0)),
                "class_id": int(d.get("cls", -1)),
                "class_name": d.get("name", "")
            })
        s["detections"] = detections
        s["det_found"] = len(detections) > 0
        
        if frame_idx % 10 == 0:
            print(f"API returned {len(detections)} objects for {model_name} (hint={det_class_hint})")
        
        return s

    def n_det_check(s):
        """æ£€æµ‹ç»“æœæ£€æŸ¥"""
        if not s.get("det_found", False):
            s["missing_gap"] = s.get("missing_gap", 0) + 1
        else:
            s["missing_gap"] = 0
        return s

    def n_bytetrack(s):
        """ByteTrackè·Ÿè¸ªèŠ‚ç‚¹"""
        frame_idx = s.get("frame_idx", 0)
        detections = s.get("detections", [])
        
        # ä½¿ç”¨MLLMå†³å®šçš„ByteTrackå‚æ•°
        bt_config = {
            "track_thresh": s.get("track_thresh", 0.3),
            "match_thresh": s.get("match_thresh", 0.8),
            "track_buffer": s.get("track_buffer", 30)
        }
        
        frame = s.get("frame")
        tracks_result = run_bytetrack(frame, detections, bt_config, frame_idx)
        tracks = tracks_result.get("tracks", [])
        s["tracks"] = tracks
        
        if frame_idx % 10 == 0:
            print(f"[BYTE] frame={frame_idx} tracks={len(tracks)} config={bt_config}")
        
        return s

    def n_bt_check(s):
        """ByteTrackç»“æœæ£€æŸ¥"""
        tracks = s.get("tracks", [])
        prev_tracks = s.get("prev_tracks", [])
        
        # è®¡ç®—IDåˆ‡æ¢
        id_switches = 0
        if prev_tracks:
            prev_ids = {t.get("track_id") for t in prev_tracks}
            curr_ids = {t.get("track_id") for t in tracks}
            id_switches = len(prev_ids.symmetric_difference(curr_ids))
        
        s["id_switch"] = id_switches
        s["prev_tracks"] = tracks.copy()
        s["stable"] = id_switches == 0
        
        return s

    def n_clip_apply(s):
        """CLIPè¯­ä¹‰åŒ¹é…èŠ‚ç‚¹"""
        frame = s.get("frame")
        detections = s.get("detections", [])
        prompt = s.get("prompt", "")
        
        # ä½¿ç”¨MLLMå†³å®šçš„CLIPé˜ˆå€¼
        clip_thresh = s.get("clip_thresh", 0.25)
        
        # æ€§èƒ½ä¼˜åŒ–ï¼šç¼“å­˜CLIPæ¨¡å‹ç»„ä»¶
        clip_model = s.get("clip_model")
        clip_preprocess = s.get("clip_preprocess") 
        clip_tokenizer = s.get("clip_tokenizer")
        clip_device = s.get("clip_device")
        
        if frame is not None and detections:
            scores = clip_stats(
                detections, prompt, frame,
                model=clip_model, preprocess=clip_preprocess,
                tokenizer=clip_tokenizer, device=clip_device
            )
            
            # ç¼“å­˜æ¨¡å‹ç»„ä»¶ï¼ˆé¦–æ¬¡åŠ è½½åï¼‰
            if clip_model is None:
                from adapter.clip_matcher import load_clip_model
                s["clip_model"], s["clip_preprocess"], s["clip_tokenizer"], s["clip_device"] = load_clip_model()
        
        s["clip_thresh"] = clip_thresh
        return s

    def n_global_id(s):
        """å…¨å±€IDç®¡ç†èŠ‚ç‚¹"""
        tracks = s.get("tracks", [])
        dets = s.get("detections", [])
        th = float(s.get("clip_thresh", 0.0))

        def _match_det_for_track(track_bbox, detections):
            best_det, best_iou = None, 0.0
            for det in detections:
                det_bbox = det.get("bbox", [0,0,0,0])
                x1 = max(track_bbox[0], det_bbox[0])
                y1 = max(track_bbox[1], det_bbox[1])
                x2 = min(track_bbox[2], det_bbox[2])
                y2 = min(track_bbox[3], det_bbox[3])
                if x2 > x1 and y2 > y1:
                    intersection = (x2 - x1) * (y2 - y1)
                    track_area = (track_bbox[2] - track_bbox[0]) * (track_bbox[3] - track_bbox[1])
                    det_area = (det_bbox[2] - det_bbox[0]) * (det_bbox[3] - det_bbox[1])
                    union = track_area + det_area - intersection
                    iou = intersection / max(union, 1e-6)
                    if iou > best_iou:
                        best_iou = iou
                        best_det = det
            return best_det if best_iou > 0.3 else None

        for t in tracks:
            tid = int(t["track_id"])
            tb = t.get("bbox", [0,0,0,0])
            d = _match_det_for_track(tb, dets)

            passed = False
            if d is not None:
                sc = float(d.get("clip_score", d.get("confidence", d.get("score", 0.0))))
                passed = (sc >= th) if th > 0 else True
            
            update_stats(tid, passed)

            # å…¨å±€IDåˆ†é…é€»è¾‘
            gid = get_assigned_gid(tid)
            if gid is None and passed and d is not None and isinstance(d.get("img_feat"), torch.Tensor):
                pidx = abs(hash(s.get("prompt",""))) % 10_000_000
                gid = get_or_assign_gid_with_reuse(tid, d["img_feat"], prompt_idx=pidx, sim_threshold=0.10)
            
            t["global_id"] = int(gid) if gid is not None else None

        if tracks:
            gids = [tr.get("global_id") for tr in tracks if tr.get("global_id") is not None]
            if gids:
                frame_idx = s.get('frame_idx', -1)
                print(f"[GID] frame={frame_idx} assigned GIDs={gids}")

        return s

    def n_edit(s):
        """ç¼–è¾‘èŠ‚ç‚¹ - ä½¿ç”¨MLLMå†³å®šçš„å‚æ•°"""
        idx = int(s.get("frame_idx", 0))
        tracks = s.get("tracks", [])
        detections = s.get("detections", [])
        gid_frames = s.get("gid_frames", {})
        gid_scores = s.get("gid_scores", {})

        # ç´¯è®¡é€šè¿‡CLIPé˜ˆå€¼çš„tracks
        for t in tracks:
            gid = t.get("global_id")
            if gid is None:
                continue
                
            track_bbox = t.get("bbox", [0,0,0,0])
            best_det = None
            best_iou = 0.0
            
            for det in detections:
                det_bbox = det.get("bbox", [0,0,0,0])
                x1, y1, x2, y2 = max(track_bbox[0], det_bbox[0]), max(track_bbox[1], det_bbox[1]), \
                                min(track_bbox[2], det_bbox[2]), min(track_bbox[3], det_bbox[3])
                if x2 > x1 and y2 > y1:
                    intersection = (x2 - x1) * (y2 - y1)
                    track_area = (track_bbox[2] - track_bbox[0]) * (track_bbox[3] - track_bbox[1])
                    det_area = (det_bbox[2] - det_bbox[0]) * (det_bbox[3] - det_bbox[1])
                    union = track_area + det_area - intersection
                    iou = intersection / max(union, 1e-6)
                    if iou > best_iou:
                        best_iou = iou
                        best_det = det
            
            if best_det is not None and best_iou > 0.3:
                clip_score = float(best_det.get("clip_score", 0.0))
                clip_thresh = float(s.get("clip_thresh", 0.0))
                passed = (clip_score >= clip_thresh) if clip_thresh > 0 else True
                
                if passed:
                    gid_frames.setdefault(int(gid), []).append(idx)
                    gid_scores.setdefault(int(gid), []).append(clip_score)
        
        s["gid_frames"] = gid_frames
        s["gid_scores"] = gid_scores

        # ä½¿ç”¨MLLMå†³å®šçš„gapå’Œbridgeå‚æ•°è¿›è¡Œç‰‡æ®µåˆå¹¶
        gap = int(s.get("gap", 2))
        bridge = int(s.get("max_bridge", 30))

        merged = {
            gid: merge_frame_ranges(frames, gap=gap, bridge=bridge)
            for gid, frames in gid_frames.items()
        }
        s["merged_segments"] = merged

        if idx % 10 == 0 and gid_frames:
            sample_gid = next(iter(gid_frames.keys()))
            sample_segments = merged.get(sample_gid, [])
            print(f"[EDIT] frame={idx} sample gid={sample_gid} segments={sample_segments}")

        return s

    def n_export(s):
        """å¯¼å‡ºèŠ‚ç‚¹ - ä»…åœ¨æœ€åä¸€å¸§æ‰§è¡Œ"""
        frame_idx = int(s.get("frame_idx", 0))
        total_frames = int(s.get("total_frames", 0))
        
        # åªåœ¨æœ€åä¸€å¸§æ‰§è¡Œå¯¼å‡º
        if frame_idx < total_frames - 1:
            return s
        
        print(f"ğŸ¬ [EXPORT] å¼€å§‹å¯¼å‡ºæœ€ç»ˆè§†é¢‘...")
        
        gid_frames = s.get("gid_frames", {})
        gid_scores = s.get("gid_scores", {})
        video_path = s.get("video_path", "")
        output_dir = s.get("output_dir", "./output")
        fps = s.get("fps", 30.0)
        
        # ä½¿ç”¨MLLMå†³å®šçš„å‚æ•°
        gap = int(s.get("gap", 2))
        bridge = int(s.get("max_bridge", 30))
        
        try:
            output_path = export_best_match_video(
                video_path=video_path,
                gid_frames=gid_frames,
                gid_scores=gid_scores,
                output_dir=output_dir,
                fps=fps,
                gap=gap,
                bridge=bridge
            )
            s["exported_video_path"] = output_path
            print(f"[SUCCESS] è§†é¢‘å¯¼å‡ºå®Œæˆ: {output_path}")
            
            # æ˜¾ç¤ºMLLMå†³ç­–ä¿¡æ¯
            mllm_confidence = s.get("mllm_confidence", 0.0)
            mllm_reasoning = s.get("mllm_reasoning", "")
            print(f"[MLLM] å‚æ•°å†³ç­–ç½®ä¿¡åº¦: {mllm_confidence}")
            print(f"[MLLM] å†³ç­–ç†ç”±: {mllm_reasoning}")
            
        except Exception as e:
            error_msg = f"è§†é¢‘å¯¼å‡ºå¤±è´¥: {str(e)}"
            s["export_error"] = error_msg
            print(f"[ERROR] {error_msg}")
        
        return s

    def n_human(s):
        """äººå·¥å®¡æ ¸èŠ‚ç‚¹"""
        s["need_human_review"] = False
        return s

    # ---------- æ¡ä»¶å‡½æ•° ----------
    def c_det(s):
        """æ£€æµ‹æ¡ä»¶åˆ¤æ–­"""
        if s.get("det_found"):
            return "ok"
        # ç®€åŒ–é€»è¾‘ï¼Œç›´æ¥ç»§ç»­å¤„ç†
        return "miss"

    def c_bt(s):
        """ByteTrackæ¡ä»¶åˆ¤æ–­"""
        return "ok"  # ç®€åŒ–ï¼Œæ€»æ˜¯ç»§ç»­

    def c_clip(s):
        """CLIPæ¡ä»¶åˆ¤æ–­"""
        return "accept"  # ç®€åŒ–ï¼Œæ€»æ˜¯æ¥å—

    # ---------- æ³¨å†ŒèŠ‚ç‚¹ ----------
    g.add_node("ParseInstruction", n_parse)
    g.add_node("MLLMAdvisor", n_mllm_advisor)  # æ–°å¢MLLMèŠ‚ç‚¹
    g.add_node("YOLODetect", n_detect)
    g.add_node("DetectResultCheck", n_det_check)
    g.add_node("ByteTrack", n_bytetrack)
    g.add_node("ByteTrackCheck", n_bt_check)
    g.add_node("CLIPApply", n_clip_apply)
    g.add_node("GlobalID", n_global_id)
    g.add_node("Edit", n_edit)
    g.add_node("Export", n_export)
    g.add_node("HumanReview", n_human)

    # ---------- è¿è¾¹ ----------
    # æ–°çš„æµç¨‹ï¼šParse -> MLLM -> Detect -> ...
    g.add_edge(START, "ParseInstruction")
    g.add_edge("ParseInstruction", "MLLMAdvisor")  # NLPåç›´æ¥è¿›å…¥MLLM
    g.add_edge("MLLMAdvisor", "YOLODetect")       # MLLMå†³ç­–åå¼€å§‹æ£€æµ‹
    g.add_edge("YOLODetect", "DetectResultCheck")
    
    g.add_conditional_edges("DetectResultCheck", c_det,
                          {"ok": "ByteTrack", "miss": "ByteTrack"})
    
    g.add_edge("ByteTrack", "ByteTrackCheck")
    g.add_conditional_edges("ByteTrackCheck", c_bt,
                          {"ok": "CLIPApply"})
    
    g.add_edge("CLIPApply", "GlobalID")
    g.add_conditional_edges("GlobalID", c_clip,
                          {"accept": "Edit"})
    
    g.add_edge("Edit", "Export")
    g.add_edge("Export", END)

    return g.compile()

# ä¸ºäº†ä¿æŒå…¼å®¹æ€§ï¼Œä¹Ÿæä¾›åŸæœ‰æ¥å£
def build_app():
    """å…¼å®¹æ€§æ¥å£ï¼Œè¿”å›MLLMç‰ˆæœ¬"""
    return build_mllm_app()
